{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function\n",
    "# import argparse\n",
    "# import glob\n",
    "# import logging\n",
    "# import os\n",
    "# import pickle\n",
    "# import random\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup,\n",
    "#                           RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "# from tqdm import tqdm\n",
    "# import multiprocessing\n",
    "# import pandas as pd\n",
    "# # metrics\n",
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, PrecisionRecallDisplay\n",
    "# from sklearn.metrics import auc\n",
    "# import matplotlib.pyplot as plt\n",
    "# # model reasoning\n",
    "# from captum.attr import LayerIntegratedGradients, DeepLift, DeepLiftShap, GradientShap, Saliency\n",
    "# # word-level tokenizer\n",
    "# from tokenizers import Tokenizer\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# llm = LLM(model=\"microsoft/codebert-base\")\n",
    "# (output,) = llm.embed(\"Hello, my name is\")\n",
    "# embeds = output.outputs.embedding\n",
    "# print(f\"Embeddings: {embeds!r} (size={len(embeds)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87687 87687\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n",
      "0\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/train.csv\")\n",
    "train_data = train_data[train_data[\"processed_func\"].str.len() < 500]\n",
    "funcs = train_data[\"processed_func\"].tolist()\n",
    "labels = train_data[\"target\"].tolist()\n",
    "X_train = funcs\n",
    "y_train = labels\n",
    "print(len(X_train), len(y_train))\n",
    "id = 0\n",
    "print(X_train[id])\n",
    "print(y_train[id])\n",
    "print(train_data[\"vul_func_with_fix\"].iloc[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32225/56384999.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processed_func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_32225/56384999.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# data = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processed_func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE ID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCWEs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE-ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processed_func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.11.0/envs/devsec/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df[\"CWE target\"] = np.nan\n",
    "# df.loc[df[\"target\"] == 1, \"CWE target\"] = df[\"CWE ID\"]\n",
    "# df = df[df[\"CWE target\"].isin(df[\"CWE target\"].value_counts(dropna=False)[df[\"CWE target\"].value_counts(dropna=False) > 10].index)]\n",
    "# df[\"CWE target\"].value_counts().index\n",
    "CWEs = ['CWE-119', 'CWE-20', 'CWE-399', 'CWE-264', 'CWE-200', 'CWE-190',\n",
    "       'CWE-416', 'CWE-125', 'CWE-189', 'CWE-362', 'CWE-284', 'CWE-254',\n",
    "       'CWE-476', 'CWE-787', 'CWE-732', 'CWE-310', 'CWE-17', 'CWE-18',\n",
    "       'CWE-404', 'CWE-79']\n",
    "assert len(CWEs) == 20\n",
    "\n",
    "def load_data(path:str):\n",
    "        # data = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[df[\"processed_func\"].str.len() < 500]\n",
    "        df = df[((df[\"CWE ID\"] in CWEs) & (df[\"target\"] == 1)) | (df[\"target\"] == 0)]\n",
    "        df[\"CWE target\"] = np.where(df[\"target\"] == 1, df[\"CWE-ID\"], np.nan)\n",
    "        X = df[\"processed_func\"].tolist()\n",
    "        y = df[\"CWE ID\"].tolist()\n",
    "        return X, y\n",
    "\n",
    "load_data(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_data\n",
    "X, y = load_data(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 11,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 10,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 16,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 17,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 2,\n",
       " 11,\n",
       " 2,\n",
       " 14,\n",
       " 2,\n",
       " 16,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 17,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 11,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 18,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 18,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 17,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 16,\n",
       " 1,\n",
       " 11,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 13,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 10,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 17,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 16,\n",
       " 7,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN        85070\n",
       "CWE-119      528\n",
       "CWE-20       248\n",
       "CWE-399      208\n",
       "CWE-264      153\n",
       "CWE-200      114\n",
       "CWE-190       82\n",
       "CWE-416       80\n",
       "CWE-125       75\n",
       "CWE-189       72\n",
       "CWE-362       59\n",
       "CWE-284       52\n",
       "CWE-254       38\n",
       "CWE-476       37\n",
       "CWE-787       36\n",
       "CWE-732       23\n",
       "CWE-310       21\n",
       "CWE-17        20\n",
       "CWE-18        18\n",
       "CWE-404       17\n",
       "CWE-79        16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(y).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CWE-119</th>\n",
       "      <td>10932</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-125</th>\n",
       "      <td>2768</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-17</th>\n",
       "      <td>298</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-189</th>\n",
       "      <td>3032</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-190</th>\n",
       "      <td>1477</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-20</th>\n",
       "      <td>9569</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-200</th>\n",
       "      <td>3452</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-254</th>\n",
       "      <td>1537</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-264</th>\n",
       "      <td>5879</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-284</th>\n",
       "      <td>1086</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-310</th>\n",
       "      <td>657</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-362</th>\n",
       "      <td>2635</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-399</th>\n",
       "      <td>7381</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-404</th>\n",
       "      <td>449</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-416</th>\n",
       "      <td>4555</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-476</th>\n",
       "      <td>1701</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-732</th>\n",
       "      <td>830</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-787</th>\n",
       "      <td>1283</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-79</th>\n",
       "      <td>429</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1\n",
       "CWE ID             \n",
       "CWE-119  10932  528\n",
       "CWE-125   2768   75\n",
       "CWE-17     298   20\n",
       "CWE-189   3032   72\n",
       "CWE-190   1477   82\n",
       "CWE-20    9569  248\n",
       "CWE-200   3452  114\n",
       "CWE-254   1537   38\n",
       "CWE-264   5879  153\n",
       "CWE-284   1086   52\n",
       "CWE-310    657   21\n",
       "CWE-362   2635   59\n",
       "CWE-399   7381  208\n",
       "CWE-404    449   17\n",
       "CWE-416   4555   80\n",
       "CWE-476   1701   37\n",
       "CWE-732    830   23\n",
       "CWE-787   1283   36\n",
       "CWE-79     429   16"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_data.groupby(\"CWE ID\")[\"target\"].apply(lambda x: x.value_counts()).unstack(fill_value=0)\n",
    "df[(df[1] > 10) & (df[0] > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n",
      "CWE ID: CWE-264\n",
      "@@ -170,6 +170,11 @@ static int proc_taint(struct ctl_table *table, int write,\n",
      " \t\t\t       void __user *buffer, size_t *lenp, loff_t *ppos);\n",
      " #endif\n",
      " \n",
      "+#ifdef CONFIG_PRINTK\n",
      "+static int proc_dmesg_restrict(struct ctl_table *table, int write,\n",
      "+\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n",
      "+#endif\n",
      "+\n",
      " #ifdef CONFIG_MAGIC_SYSRQ\n",
      " /* Note: sysrq code uses it's own private copy */\n",
      " static int __sysrq_enabled = SYSRQ_DEFAULT_ENABLE;\n",
      "@@ -707,7 +712,7 @@ static struct ctl_table kern_table[] = {\n",
      " \t\t.data\t\t= &kptr_restrict,\n",
      " \t\t.maxlen\t\t= sizeof(int),\n",
      " \t\t.mode\t\t= 0644,\n",
      "-\t\t.proc_handler\t= proc_dointvec_minmax,\n",
      "+\t\t.proc_handler\t= proc_dmesg_restrict,\n",
      " \t\t.extra1\t\t= &zero,\n",
      " \t\t.extra2\t\t= &two,\n",
      " \t},\n",
      "@@ -2394,6 +2399,17 @@ static int proc_taint(struct ctl_table *table, int write,\n",
      " \treturn err;\n",
      " }\n",
      " \n",
      "+#ifdef CONFIG_PRINTK\n",
      "+static int proc_dmesg_restrict(struct ctl_table *table, int write,\n",
      "+\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n",
      "+{\n",
      "+\tif (write && !capable(CAP_SYS_ADMIN))\n",
      "+\t\treturn -EPERM;\n",
      "+\n",
      "+\treturn proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n",
      "+}\n",
      "+#endif\n",
      "+\n",
      " struct do_proc_dointvec_minmax_conv_param {\n",
      " \tint *min;\n",
      " \tint *max;\n"
     ]
    }
   ],
   "source": [
    "id = 0\n",
    "print(y_train[id])\n",
    "print(X_train[id])\n",
    "print(train_data[\"vul_func_with_fix\"].iloc[id])\n",
    "print(\"CWE ID:\", train_data[\"CWE ID\"].iloc[id])\n",
    "print(train_data[\"patch\"].iloc[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 21:21:18 config.py:2272] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-22 21:21:25 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/codebert-base', speculative_config=None, tokenizer='microsoft/codebert-base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=514, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/codebert-base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-22 21:21:26 interface.py:236] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 01-22 21:21:26 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-22 21:21:27 model_runner.py:1094] Starting to load model microsoft/codebert-base...\n",
      "INFO 01-22 21:21:27 weight_utils.py:251] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d2ea8ec15b4e1b9d8a210c5eab22fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.pyenv/versions/3.11.0/envs/devsec/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py:450: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 21:21:28 model_runner.py:1099] Loading model weights took 0.2366 GB\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "llm = LLM(model=\"microsoft/codebert-base\", task=\"embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CachedRobertaTokenizerFast(name_or_path='microsoft/codebert-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.6366297104779411\n",
      "Epoch 1: loss=0.5833580051611991\n",
      "Epoch 2: loss=0.542085811563207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f40a4107740>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pawel/.pyenv/versions/3.11.0/lib/python3.11/weakref.py\", line 105, in remove\n",
      "    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.5084078275240385\n",
      "Epoch 4: loss=0.48387081094457013\n",
      "Epoch 5: loss=0.4663452700791855\n",
      "Epoch 6: loss=0.44898587740384616\n",
      "Epoch 7: loss=0.43626178724194004\n",
      "Epoch 8: loss=0.4275324066300198\n",
      "Epoch 9: loss=0.4184558713058541\n"
     ]
    }
   ],
   "source": [
    "import tinygrad.tensor\n",
    "from MyModel import MyModel, ClassificationHead\n",
    "import tinygrad\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    dropout = 0.1\n",
    "    hidden_sizes = [768, 256, 32]\n",
    "    num_labels = 2\n",
    "\n",
    "config = Config()\n",
    "myModel = MyModel(llm, config)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i + batch_size]\n",
    "        batch_y = y_train[i:i + batch_size]\n",
    "        loss = myModel.train(batch_X, tinygrad.tensor.Tensor(batch_y))\n",
    "        losses.append(loss.mean().numpy())\n",
    "    \n",
    "    # for batch in eval_data:\n",
    "    #     myModel.eval(batch)\n",
    "\n",
    "    # myModel.save(f\"myModel-{epoch}.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={np.sum(losses)/len(losses)}\")#, acc={myModel.acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch_X\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(batch_X[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyModel' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mmyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(batch)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# for batch in eval_data:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     myModel.eval(batch)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     myModel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmyModel-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyModel' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_data:\n",
    "        myModel.train(batch)\n",
    "\n",
    "    # for batch in eval_data:\n",
    "    #     myModel.eval(batch)\n",
    "\n",
    "    myModel.save(f\"myModel-{epoch}.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={myModel.loss})\")#, acc={myModel.acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_csv('/home/pawel/devSecOps/LineVul/data/big-vul_dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Access Gained</th>\n",
       "      <th>Attack Origin</th>\n",
       "      <th>Authentication Required</th>\n",
       "      <th>Availability</th>\n",
       "      <th>CVE ID</th>\n",
       "      <th>CVE Page</th>\n",
       "      <th>CWE ID</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Confidentiality</th>\n",
       "      <th>...</th>\n",
       "      <th>parentID</th>\n",
       "      <th>patch</th>\n",
       "      <th>project</th>\n",
       "      <th>project_after</th>\n",
       "      <th>project_before</th>\n",
       "      <th>target</th>\n",
       "      <th>vul_func_with_fix</th>\n",
       "      <th>processed_func</th>\n",
       "      <th>flaw_line</th>\n",
       "      <th>flaw_line_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73752</td>\n",
       "      <td>None</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2016-6561</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2016-6561/</td>\n",
       "      <td>CWE-476</td>\n",
       "      <td>Low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -21,6 +21,7 @@\\n /*\\n  * Copyright (c) 2007...</td>\n",
       "      <td>illumos-gate</td>\n",
       "      <td>6d1c73b5858fefc6161c7d686345f0dc887ea799</td>\n",
       "      <td>516627f338a630bcf9806a91aa873bbbae9a2fac</td>\n",
       "      <td>0</td>\n",
       "      <td>smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...</td>\n",
       "      <td>smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54196</td>\n",
       "      <td>None</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2016-3138</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2016-3138/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1179,6 +1179,9 @@ static int acm_probe(str...</td>\n",
       "      <td>linux</td>\n",
       "      <td>8835ba4a39cf53f705417b3b3a94eb067673f2c9</td>\n",
       "      <td>0b818e3956fc1ad976bee791eadcbb3b5fec5bfd</td>\n",
       "      <td>0</td>\n",
       "      <td>static inline int acm_set_control(struct acm *...</td>\n",
       "      <td>static inline int acm_set_control(struct acm *...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169124</td>\n",
       "      <td>None</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>None</td>\n",
       "      <td>CVE-2018-6145</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2018-6145/</td>\n",
       "      <td>CWE-79</td>\n",
       "      <td>Medium</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -82,13 +82,6 @@ static bool TokenExitsForei...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>133bc5c262b2555af223263452e9875a95db9eb7</td>\n",
       "      <td>1e8327c88920544f1503004b4e32850c935d4efb</td>\n",
       "      <td>0</td>\n",
       "      <td>HTMLTreeBuilderSimulator::State HTMLTreeBuilde...</td>\n",
       "      <td>HTMLTreeBuilderSimulator::State HTMLTreeBuilde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109551</td>\n",
       "      <td>None</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Partial</td>\n",
       "      <td>CVE-2012-5135</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2012-5135/</td>\n",
       "      <td>CWE-399</td>\n",
       "      <td>Low</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -713,7 +713,8 @@ PrintWebViewHelper::PrintW...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>b755ebba29dd405d6f1e4cf70f5bc81ffd33b0f6</td>\n",
       "      <td>7b688dec9fa8ab42a4933e381ad9aeb63413139b</td>\n",
       "      <td>0</td>\n",
       "      <td>int PrintWebViewHelper::PrintPreviewContext::t...</td>\n",
       "      <td>int PrintWebViewHelper::PrintPreviewContext::t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78906</td>\n",
       "      <td>None</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2018-16276</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2018-16276/</td>\n",
       "      <td>CWE-20</td>\n",
       "      <td>Low</td>\n",
       "      <td>Complete</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -396,35 +396,24 @@ static ssize_t yurex_rea...</td>\n",
       "      <td>linux</td>\n",
       "      <td>f1e255d60ae66a9f672ff9a207ee6cd8e33d2679</td>\n",
       "      <td>bba57eddadda936c94b5dccf73787cb9e159d0a5</td>\n",
       "      <td>0</td>\n",
       "      <td>static int yurex_open(struct inode *inode, str...</td>\n",
       "      <td>static int yurex_open(struct inode *inode, str...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
       "0   73752          None        Remote            Not required     Complete   \n",
       "1   54196          None         Local            Not required     Complete   \n",
       "2  169124          None        Remote            Not required         None   \n",
       "3  109551          None        Remote            Not required      Partial   \n",
       "4   78906          None         Local            Not required     Complete   \n",
       "\n",
       "           CVE ID                                        CVE Page   CWE ID  \\\n",
       "0   CVE-2016-6561   https://www.cvedetails.com/cve/CVE-2016-6561/  CWE-476   \n",
       "1   CVE-2016-3138   https://www.cvedetails.com/cve/CVE-2016-3138/      NaN   \n",
       "2   CVE-2018-6145   https://www.cvedetails.com/cve/CVE-2018-6145/   CWE-79   \n",
       "3   CVE-2012-5135   https://www.cvedetails.com/cve/CVE-2012-5135/  CWE-399   \n",
       "4  CVE-2018-16276  https://www.cvedetails.com/cve/CVE-2018-16276/   CWE-20   \n",
       "\n",
       "  Complexity Confidentiality  ... parentID  \\\n",
       "0        Low            None  ...      NaN   \n",
       "1        Low            None  ...      NaN   \n",
       "2     Medium            None  ...      NaN   \n",
       "3        Low         Partial  ...      NaN   \n",
       "4        Low        Complete  ...      NaN   \n",
       "\n",
       "                                               patch       project  \\\n",
       "0  @@ -21,6 +21,7 @@\\n /*\\n  * Copyright (c) 2007...  illumos-gate   \n",
       "1  @@ -1179,6 +1179,9 @@ static int acm_probe(str...         linux   \n",
       "2  @@ -82,13 +82,6 @@ static bool TokenExitsForei...        Chrome   \n",
       "3  @@ -713,7 +713,8 @@ PrintWebViewHelper::PrintW...        Chrome   \n",
       "4  @@ -396,35 +396,24 @@ static ssize_t yurex_rea...         linux   \n",
       "\n",
       "                              project_after  \\\n",
       "0  6d1c73b5858fefc6161c7d686345f0dc887ea799   \n",
       "1  8835ba4a39cf53f705417b3b3a94eb067673f2c9   \n",
       "2  133bc5c262b2555af223263452e9875a95db9eb7   \n",
       "3  b755ebba29dd405d6f1e4cf70f5bc81ffd33b0f6   \n",
       "4  f1e255d60ae66a9f672ff9a207ee6cd8e33d2679   \n",
       "\n",
       "                             project_before target  \\\n",
       "0  516627f338a630bcf9806a91aa873bbbae9a2fac      0   \n",
       "1  0b818e3956fc1ad976bee791eadcbb3b5fec5bfd      0   \n",
       "2  1e8327c88920544f1503004b4e32850c935d4efb      0   \n",
       "3  7b688dec9fa8ab42a4933e381ad9aeb63413139b      0   \n",
       "4  bba57eddadda936c94b5dccf73787cb9e159d0a5      0   \n",
       "\n",
       "                                   vul_func_with_fix  \\\n",
       "0  smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...   \n",
       "1  static inline int acm_set_control(struct acm *...   \n",
       "2  HTMLTreeBuilderSimulator::State HTMLTreeBuilde...   \n",
       "3  int PrintWebViewHelper::PrintPreviewContext::t...   \n",
       "4  static int yurex_open(struct inode *inode, str...   \n",
       "\n",
       "                                      processed_func flaw_line flaw_line_index  \n",
       "0  smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...       NaN             NaN  \n",
       "1  static inline int acm_set_control(struct acm *...       NaN             NaN  \n",
       "2  HTMLTreeBuilderSimulator::State HTMLTreeBuilde...       NaN             NaN  \n",
       "3  int PrintWebViewHelper::PrintPreviewContext::t...       NaN             NaN  \n",
       "4  static int yurex_open(struct inode *inode, str...       NaN             NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, eval_dataset):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # build dataloader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n",
    "    \n",
    "    args.max_steps = args.epochs * len(train_dataloader)\n",
    "    # evaluate the model per epoch\n",
    "    args.save_steps = len(train_dataloader)\n",
    "    args.warmup_steps = args.max_steps // 5\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=args.max_steps)\n",
    "\n",
    "    # multi-gpu training\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size//max(args.n_gpu, 1))\n",
    "    logger.info(\"  Total train batch size = %d\",args.train_batch_size*args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", args.max_steps)\n",
    "    \n",
    "    global_step=0\n",
    "    tr_loss, logging_loss, avg_loss, tr_nb, tr_num, train_loss = 0.0, 0.0, 0.0, 0, 0, 0\n",
    "    best_f1=0\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for idx in range(args.epochs): \n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        tr_num = 0\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(bar):\n",
    "            (inputs_ids, labels) = [x.to(args.device) for x in batch]\n",
    "            model.train()\n",
    "            loss, logits = model(input_ids=inputs_ids, labels=labels)\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_num += 1\n",
    "            train_loss += loss.item()\n",
    "            if avg_loss == 0:\n",
    "                avg_loss = tr_loss\n",
    "                \n",
    "            avg_loss = round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "              \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()  \n",
    "                global_step += 1\n",
    "                output_flag=True\n",
    "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "\n",
    "                if global_step % args.save_steps == 0:\n",
    "                    results = evaluate(args, model, tokenizer, eval_dataset, eval_when_training=True)    \n",
    "                    \n",
    "                    # Save model checkpoint\n",
    "                    if results['eval_f1']>best_f1:\n",
    "                        best_f1=results['eval_f1']\n",
    "                        logger.info(\"  \"+\"*\"*20)  \n",
    "                        logger.info(\"  Best f1:%s\",round(best_f1,4))\n",
    "                        logger.info(\"  \"+\"*\"*20)                          \n",
    "                        \n",
    "                        checkpoint_prefix = 'checkpoint-best-f1'\n",
    "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))                        \n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)                        \n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format(args.model_name)) \n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devsec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
