{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function\n",
    "# import argparse\n",
    "# import glob\n",
    "# import logging\n",
    "# import os\n",
    "# import pickle\n",
    "# import random\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup,\n",
    "#                           RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "# from tqdm import tqdm\n",
    "# import multiprocessing\n",
    "# import pandas as pd\n",
    "# # metrics\n",
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, PrecisionRecallDisplay\n",
    "# from sklearn.metrics import auc\n",
    "# import matplotlib.pyplot as plt\n",
    "# # model reasoning\n",
    "# from captum.attr import LayerIntegratedGradients, DeepLift, DeepLiftShap, GradientShap, Saliency\n",
    "# # word-level tokenizer\n",
    "# from tokenizers import Tokenizer\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# llm = LLM(model=\"microsoft/codebert-base\")\n",
    "# (output,) = llm.embed(\"Hello, my name is\")\n",
    "# embeds = output.outputs.embedding\n",
    "# print(f\"Embeddings: {embeds!r} (size={len(embeds)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87687 87687\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n",
      "0\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/train.csv\")\n",
    "train_data = train_data[train_data[\"processed_func\"].str.len() < 500]\n",
    "funcs = train_data[\"processed_func\"].tolist()\n",
    "labels = train_data[\"target\"].tolist()\n",
    "X_train = funcs\n",
    "y_train = labels\n",
    "print(len(X_train), len(y_train))\n",
    "id = 0\n",
    "print(X_train[id])\n",
    "print(y_train[id])\n",
    "print(train_data[\"vul_func_with_fix\"].iloc[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32225/56384999.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processed_func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_32225/56384999.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# data = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processed_func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE ID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCWEs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE-ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processed_func\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CWE ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.11.0/envs/devsec/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df[\"CWE target\"] = np.nan\n",
    "# df.loc[df[\"target\"] == 1, \"CWE target\"] = df[\"CWE ID\"]\n",
    "# df = df[df[\"CWE target\"].isin(df[\"CWE target\"].value_counts(dropna=False)[df[\"CWE target\"].value_counts(dropna=False) > 10].index)]\n",
    "# df[\"CWE target\"].value_counts().index\n",
    "CWEs = ['CWE-119', 'CWE-20', 'CWE-399', 'CWE-264', 'CWE-200', 'CWE-190',\n",
    "       'CWE-416', 'CWE-125', 'CWE-189', 'CWE-362', 'CWE-284', 'CWE-254',\n",
    "       'CWE-476', 'CWE-787', 'CWE-732', 'CWE-310', 'CWE-17', 'CWE-18',\n",
    "       'CWE-404', 'CWE-79']\n",
    "assert len(CWEs) == 20\n",
    "\n",
    "def load_data(path:str):\n",
    "        # data = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")\n",
    "        df = pd.read_csv(path)\n",
    "        df = df[df[\"processed_func\"].str.len() < 500]\n",
    "        df = df[((df[\"CWE ID\"] in CWEs) & (df[\"target\"] == 1)) | (df[\"target\"] == 0)]\n",
    "        df[\"CWE target\"] = np.where(df[\"target\"] == 1, df[\"CWE-ID\"], np.nan)\n",
    "        X = df[\"processed_func\"].tolist()\n",
    "        y = df[\"CWE ID\"].tolist()\n",
    "        return X, y\n",
    "\n",
    "load_data(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_data\n",
    "X, y = load_data(\"~/devSecOps/LineVul/data/big-vul_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 11,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 10,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 16,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 17,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 2,\n",
       " 11,\n",
       " 2,\n",
       " 14,\n",
       " 2,\n",
       " 16,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 17,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 11,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 18,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 18,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 17,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 16,\n",
       " 1,\n",
       " 11,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 13,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 10,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 17,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 16,\n",
       " 7,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN        85070\n",
       "CWE-119      528\n",
       "CWE-20       248\n",
       "CWE-399      208\n",
       "CWE-264      153\n",
       "CWE-200      114\n",
       "CWE-190       82\n",
       "CWE-416       80\n",
       "CWE-125       75\n",
       "CWE-189       72\n",
       "CWE-362       59\n",
       "CWE-284       52\n",
       "CWE-254       38\n",
       "CWE-476       37\n",
       "CWE-787       36\n",
       "CWE-732       23\n",
       "CWE-310       21\n",
       "CWE-17        20\n",
       "CWE-18        18\n",
       "CWE-404       17\n",
       "CWE-79        16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(y).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CWE-119</th>\n",
       "      <td>10932</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-125</th>\n",
       "      <td>2768</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-17</th>\n",
       "      <td>298</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-189</th>\n",
       "      <td>3032</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-190</th>\n",
       "      <td>1477</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-20</th>\n",
       "      <td>9569</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-200</th>\n",
       "      <td>3452</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-254</th>\n",
       "      <td>1537</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-264</th>\n",
       "      <td>5879</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-284</th>\n",
       "      <td>1086</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-310</th>\n",
       "      <td>657</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-362</th>\n",
       "      <td>2635</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-399</th>\n",
       "      <td>7381</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-404</th>\n",
       "      <td>449</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-416</th>\n",
       "      <td>4555</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-476</th>\n",
       "      <td>1701</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-732</th>\n",
       "      <td>830</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-787</th>\n",
       "      <td>1283</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE-79</th>\n",
       "      <td>429</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1\n",
       "CWE ID             \n",
       "CWE-119  10932  528\n",
       "CWE-125   2768   75\n",
       "CWE-17     298   20\n",
       "CWE-189   3032   72\n",
       "CWE-190   1477   82\n",
       "CWE-20    9569  248\n",
       "CWE-200   3452  114\n",
       "CWE-254   1537   38\n",
       "CWE-264   5879  153\n",
       "CWE-284   1086   52\n",
       "CWE-310    657   21\n",
       "CWE-362   2635   59\n",
       "CWE-399   7381  208\n",
       "CWE-404    449   17\n",
       "CWE-416   4555   80\n",
       "CWE-476   1701   37\n",
       "CWE-732    830   23\n",
       "CWE-787   1283   36\n",
       "CWE-79     429   16"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_data.groupby(\"CWE ID\")[\"target\"].apply(lambda x: x.value_counts()).unstack(fill_value=0)\n",
    "df[(df[1] > 10) & (df[0] > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n",
      "void register_sysctl_root(struct ctl_table_root *root)\n",
      "{\n",
      "\tspin_lock(&sysctl_lock);\n",
      "\tlist_add_tail(&root->root_list, &sysctl_table_root.root_list);\n",
      "\tspin_unlock(&sysctl_lock);\n",
      "}\n",
      "\n",
      "CWE ID: CWE-264\n",
      "@@ -170,6 +170,11 @@ static int proc_taint(struct ctl_table *table, int write,\n",
      " \t\t\t       void __user *buffer, size_t *lenp, loff_t *ppos);\n",
      " #endif\n",
      " \n",
      "+#ifdef CONFIG_PRINTK\n",
      "+static int proc_dmesg_restrict(struct ctl_table *table, int write,\n",
      "+\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n",
      "+#endif\n",
      "+\n",
      " #ifdef CONFIG_MAGIC_SYSRQ\n",
      " /* Note: sysrq code uses it's own private copy */\n",
      " static int __sysrq_enabled = SYSRQ_DEFAULT_ENABLE;\n",
      "@@ -707,7 +712,7 @@ static struct ctl_table kern_table[] = {\n",
      " \t\t.data\t\t= &kptr_restrict,\n",
      " \t\t.maxlen\t\t= sizeof(int),\n",
      " \t\t.mode\t\t= 0644,\n",
      "-\t\t.proc_handler\t= proc_dointvec_minmax,\n",
      "+\t\t.proc_handler\t= proc_dmesg_restrict,\n",
      " \t\t.extra1\t\t= &zero,\n",
      " \t\t.extra2\t\t= &two,\n",
      " \t},\n",
      "@@ -2394,6 +2399,17 @@ static int proc_taint(struct ctl_table *table, int write,\n",
      " \treturn err;\n",
      " }\n",
      " \n",
      "+#ifdef CONFIG_PRINTK\n",
      "+static int proc_dmesg_restrict(struct ctl_table *table, int write,\n",
      "+\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n",
      "+{\n",
      "+\tif (write && !capable(CAP_SYS_ADMIN))\n",
      "+\t\treturn -EPERM;\n",
      "+\n",
      "+\treturn proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n",
      "+}\n",
      "+#endif\n",
      "+\n",
      " struct do_proc_dointvec_minmax_conv_param {\n",
      " \tint *min;\n",
      " \tint *max;\n"
     ]
    }
   ],
   "source": [
    "id = 0\n",
    "print(y_train[id])\n",
    "print(X_train[id])\n",
    "print(train_data[\"vul_func_with_fix\"].iloc[id])\n",
    "print(\"CWE ID:\", train_data[\"CWE ID\"].iloc[id])\n",
    "print(train_data[\"patch\"].iloc[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 21:21:18 config.py:2272] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-22 21:21:25 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/codebert-base', speculative_config=None, tokenizer='microsoft/codebert-base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=514, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/codebert-base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-22 21:21:26 interface.py:236] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 01-22 21:21:26 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-22 21:21:27 model_runner.py:1094] Starting to load model microsoft/codebert-base...\n",
      "INFO 01-22 21:21:27 weight_utils.py:251] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d2ea8ec15b4e1b9d8a210c5eab22fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/.pyenv/versions/3.11.0/envs/devsec/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py:450: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 21:21:28 model_runner.py:1099] Loading model weights took 0.2366 GB\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "llm = LLM(model=\"microsoft/codebert-base\", task=\"embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CachedRobertaTokenizerFast(name_or_path='microsoft/codebert-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.6366297104779411\n",
      "Epoch 1: loss=0.5833580051611991\n",
      "Epoch 2: loss=0.542085811563207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f40a4107740>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pawel/.pyenv/versions/3.11.0/lib/python3.11/weakref.py\", line 105, in remove\n",
      "    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.5084078275240385\n",
      "Epoch 4: loss=0.48387081094457013\n",
      "Epoch 5: loss=0.4663452700791855\n",
      "Epoch 6: loss=0.44898587740384616\n",
      "Epoch 7: loss=0.43626178724194004\n",
      "Epoch 8: loss=0.4275324066300198\n",
      "Epoch 9: loss=0.4184558713058541\n"
     ]
    }
   ],
   "source": [
    "import tinygrad.tensor\n",
    "from MyModel import MyModel, ClassificationHead\n",
    "import tinygrad\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    dropout = 0.1\n",
    "    hidden_sizes = [768, 256, 32]\n",
    "    num_labels = 2\n",
    "\n",
    "config = Config()\n",
    "myModel = MyModel(llm, config)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i + batch_size]\n",
    "        batch_y = y_train[i:i + batch_size]\n",
    "        loss = myModel.train(batch_X, tinygrad.tensor.Tensor(batch_y))\n",
    "        losses.append(loss.mean().numpy())\n",
    "    \n",
    "    # for batch in eval_data:\n",
    "    #     myModel.eval(batch)\n",
    "\n",
    "    # myModel.save(f\"myModel-{epoch}.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={np.sum(losses)/len(losses)}\")#, acc={myModel.acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch_X\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(batch_X[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyModel' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mmyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(batch)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# for batch in eval_data:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     myModel.eval(batch)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     myModel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmyModel-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyModel' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_data:\n",
    "        myModel.train(batch)\n",
    "\n",
    "    # for batch in eval_data:\n",
    "    #     myModel.eval(batch)\n",
    "\n",
    "    myModel.save(f\"myModel-{epoch}.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={myModel.loss})\")#, acc={myModel.acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_csv('/home/pawel/devSecOps/LineVul/data/big-vul_dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Access Gained</th>\n",
       "      <th>Attack Origin</th>\n",
       "      <th>Authentication Required</th>\n",
       "      <th>Availability</th>\n",
       "      <th>CVE ID</th>\n",
       "      <th>CVE Page</th>\n",
       "      <th>CWE ID</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Confidentiality</th>\n",
       "      <th>...</th>\n",
       "      <th>parentID</th>\n",
       "      <th>patch</th>\n",
       "      <th>project</th>\n",
       "      <th>project_after</th>\n",
       "      <th>project_before</th>\n",
       "      <th>target</th>\n",
       "      <th>vul_func_with_fix</th>\n",
       "      <th>processed_func</th>\n",
       "      <th>flaw_line</th>\n",
       "      <th>flaw_line_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73752</td>\n",
       "      <td>None</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2016-6561</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2016-6561/</td>\n",
       "      <td>CWE-476</td>\n",
       "      <td>Low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -21,6 +21,7 @@\\n /*\\n  * Copyright (c) 2007...</td>\n",
       "      <td>illumos-gate</td>\n",
       "      <td>6d1c73b5858fefc6161c7d686345f0dc887ea799</td>\n",
       "      <td>516627f338a630bcf9806a91aa873bbbae9a2fac</td>\n",
       "      <td>0</td>\n",
       "      <td>smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...</td>\n",
       "      <td>smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54196</td>\n",
       "      <td>None</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2016-3138</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2016-3138/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1179,6 +1179,9 @@ static int acm_probe(str...</td>\n",
       "      <td>linux</td>\n",
       "      <td>8835ba4a39cf53f705417b3b3a94eb067673f2c9</td>\n",
       "      <td>0b818e3956fc1ad976bee791eadcbb3b5fec5bfd</td>\n",
       "      <td>0</td>\n",
       "      <td>static inline int acm_set_control(struct acm *...</td>\n",
       "      <td>static inline int acm_set_control(struct acm *...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169124</td>\n",
       "      <td>None</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>None</td>\n",
       "      <td>CVE-2018-6145</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2018-6145/</td>\n",
       "      <td>CWE-79</td>\n",
       "      <td>Medium</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -82,13 +82,6 @@ static bool TokenExitsForei...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>133bc5c262b2555af223263452e9875a95db9eb7</td>\n",
       "      <td>1e8327c88920544f1503004b4e32850c935d4efb</td>\n",
       "      <td>0</td>\n",
       "      <td>HTMLTreeBuilderSimulator::State HTMLTreeBuilde...</td>\n",
       "      <td>HTMLTreeBuilderSimulator::State HTMLTreeBuilde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109551</td>\n",
       "      <td>None</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Partial</td>\n",
       "      <td>CVE-2012-5135</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2012-5135/</td>\n",
       "      <td>CWE-399</td>\n",
       "      <td>Low</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -713,7 +713,8 @@ PrintWebViewHelper::PrintW...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>b755ebba29dd405d6f1e4cf70f5bc81ffd33b0f6</td>\n",
       "      <td>7b688dec9fa8ab42a4933e381ad9aeb63413139b</td>\n",
       "      <td>0</td>\n",
       "      <td>int PrintWebViewHelper::PrintPreviewContext::t...</td>\n",
       "      <td>int PrintWebViewHelper::PrintPreviewContext::t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78906</td>\n",
       "      <td>None</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2018-16276</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2018-16276/</td>\n",
       "      <td>CWE-20</td>\n",
       "      <td>Low</td>\n",
       "      <td>Complete</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -396,35 +396,24 @@ static ssize_t yurex_rea...</td>\n",
       "      <td>linux</td>\n",
       "      <td>f1e255d60ae66a9f672ff9a207ee6cd8e33d2679</td>\n",
       "      <td>bba57eddadda936c94b5dccf73787cb9e159d0a5</td>\n",
       "      <td>0</td>\n",
       "      <td>static int yurex_open(struct inode *inode, str...</td>\n",
       "      <td>static int yurex_open(struct inode *inode, str...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
       "0   73752          None        Remote            Not required     Complete   \n",
       "1   54196          None         Local            Not required     Complete   \n",
       "2  169124          None        Remote            Not required         None   \n",
       "3  109551          None        Remote            Not required      Partial   \n",
       "4   78906          None         Local            Not required     Complete   \n",
       "\n",
       "           CVE ID                                        CVE Page   CWE ID  \\\n",
       "0   CVE-2016-6561   https://www.cvedetails.com/cve/CVE-2016-6561/  CWE-476   \n",
       "1   CVE-2016-3138   https://www.cvedetails.com/cve/CVE-2016-3138/      NaN   \n",
       "2   CVE-2018-6145   https://www.cvedetails.com/cve/CVE-2018-6145/   CWE-79   \n",
       "3   CVE-2012-5135   https://www.cvedetails.com/cve/CVE-2012-5135/  CWE-399   \n",
       "4  CVE-2018-16276  https://www.cvedetails.com/cve/CVE-2018-16276/   CWE-20   \n",
       "\n",
       "  Complexity Confidentiality  ... parentID  \\\n",
       "0        Low            None  ...      NaN   \n",
       "1        Low            None  ...      NaN   \n",
       "2     Medium            None  ...      NaN   \n",
       "3        Low         Partial  ...      NaN   \n",
       "4        Low        Complete  ...      NaN   \n",
       "\n",
       "                                               patch       project  \\\n",
       "0  @@ -21,6 +21,7 @@\\n /*\\n  * Copyright (c) 2007...  illumos-gate   \n",
       "1  @@ -1179,6 +1179,9 @@ static int acm_probe(str...         linux   \n",
       "2  @@ -82,13 +82,6 @@ static bool TokenExitsForei...        Chrome   \n",
       "3  @@ -713,7 +713,8 @@ PrintWebViewHelper::PrintW...        Chrome   \n",
       "4  @@ -396,35 +396,24 @@ static ssize_t yurex_rea...         linux   \n",
       "\n",
       "                              project_after  \\\n",
       "0  6d1c73b5858fefc6161c7d686345f0dc887ea799   \n",
       "1  8835ba4a39cf53f705417b3b3a94eb067673f2c9   \n",
       "2  133bc5c262b2555af223263452e9875a95db9eb7   \n",
       "3  b755ebba29dd405d6f1e4cf70f5bc81ffd33b0f6   \n",
       "4  f1e255d60ae66a9f672ff9a207ee6cd8e33d2679   \n",
       "\n",
       "                             project_before target  \\\n",
       "0  516627f338a630bcf9806a91aa873bbbae9a2fac      0   \n",
       "1  0b818e3956fc1ad976bee791eadcbb3b5fec5bfd      0   \n",
       "2  1e8327c88920544f1503004b4e32850c935d4efb      0   \n",
       "3  7b688dec9fa8ab42a4933e381ad9aeb63413139b      0   \n",
       "4  bba57eddadda936c94b5dccf73787cb9e159d0a5      0   \n",
       "\n",
       "                                   vul_func_with_fix  \\\n",
       "0  smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...   \n",
       "1  static inline int acm_set_control(struct acm *...   \n",
       "2  HTMLTreeBuilderSimulator::State HTMLTreeBuilde...   \n",
       "3  int PrintWebViewHelper::PrintPreviewContext::t...   \n",
       "4  static int yurex_open(struct inode *inode, str...   \n",
       "\n",
       "                                      processed_func flaw_line flaw_line_index  \n",
       "0  smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...       NaN             NaN  \n",
       "1  static inline int acm_set_control(struct acm *...       NaN             NaN  \n",
       "2  HTMLTreeBuilderSimulator::State HTMLTreeBuilde...       NaN             NaN  \n",
       "3  int PrintWebViewHelper::PrintPreviewContext::t...       NaN             NaN  \n",
       "4  static int yurex_open(struct inode *inode, str...       NaN             NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, eval_dataset):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # build dataloader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n",
    "    \n",
    "    args.max_steps = args.epochs * len(train_dataloader)\n",
    "    # evaluate the model per epoch\n",
    "    args.save_steps = len(train_dataloader)\n",
    "    args.warmup_steps = args.max_steps // 5\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=args.max_steps)\n",
    "\n",
    "    # multi-gpu training\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size//max(args.n_gpu, 1))\n",
    "    logger.info(\"  Total train batch size = %d\",args.train_batch_size*args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", args.max_steps)\n",
    "    \n",
    "    global_step=0\n",
    "    tr_loss, logging_loss, avg_loss, tr_nb, tr_num, train_loss = 0.0, 0.0, 0.0, 0, 0, 0\n",
    "    best_f1=0\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for idx in range(args.epochs): \n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        tr_num = 0\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(bar):\n",
    "            (inputs_ids, labels) = [x.to(args.device) for x in batch]\n",
    "            model.train()\n",
    "            loss, logits = model(input_ids=inputs_ids, labels=labels)\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_num += 1\n",
    "            train_loss += loss.item()\n",
    "            if avg_loss == 0:\n",
    "                avg_loss = tr_loss\n",
    "                \n",
    "            avg_loss = round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "              \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()  \n",
    "                global_step += 1\n",
    "                output_flag=True\n",
    "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "\n",
    "                if global_step % args.save_steps == 0:\n",
    "                    results = evaluate(args, model, tokenizer, eval_dataset, eval_when_training=True)    \n",
    "                    \n",
    "                    # Save model checkpoint\n",
    "                    if results['eval_f1']>best_f1:\n",
    "                        best_f1=results['eval_f1']\n",
    "                        logger.info(\"  \"+\"*\"*20)  \n",
    "                        logger.info(\"  Best f1:%s\",round(best_f1,4))\n",
    "                        logger.info(\"  \"+\"*\"*20)                          \n",
    "                        \n",
    "                        checkpoint_prefix = 'checkpoint-best-f1'\n",
    "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))                        \n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)                        \n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format(args.model_name)) \n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devsec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
